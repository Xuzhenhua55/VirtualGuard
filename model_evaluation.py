"""
Model Evaluation Script for Watermark Defense
é’ˆå¯¹è®­ç»ƒå®Œæˆçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬BLEU-4ã€ROUGE-2å’ŒLLM-as-JudgeæŒ‡æ ‡
"""

import os
import json
import random
import argparse
from typing import List, Dict, Tuple, Optional, Any
from tqdm import tqdm
import numpy as np

# è¯„ä¼°æŒ‡æ ‡
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer

# æ¨¡å‹åŠ è½½
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    GenerationConfig
)
from peft import PeftModel

# OpenAI API
import openai
from openai import OpenAI

# æ•°æ®é›†åŠ è½½
from dataset_creation import load_custom_dataset
from watermark_encoder import WatermarkEncoder
from llm_refusal_detector import LLMRefusalDetector

# å…¨å±€å¼€å…³ï¼šæ˜¯å¦æš‚æ—¶å…³é—­ n-gram ç±»æŒ‡æ ‡ï¼ˆBLEU-4ã€ROUGE-2ï¼‰
DISABLE_NGRAM_METRICS = True

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 base_model_path: str,
                 adapter_path: Optional[str] = None,
                 openai_api_key: Optional[str] = None,
                 device: str = "auto",
                 watermark_message: Optional[str] = None,
                 watermark_seed: int = 42,
                 refusal_detector_model_path: Optional[str] = None,
                 refusal_device: Optional[str] = None,
                 use_openai_refusal_detection: bool = False,
                 refusal_openai_model: str = "gpt-4o-mini",
                 target_method: str = "VirtualGuard"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            adapter_path: LoRAé€‚é…å™¨è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            openai_api_key: OpenAI APIå¯†é’¥
            device: è®¾å¤‡ç±»å‹
            watermark_message: watermarkæ¶ˆæ¯
            watermark_seed: watermarkç¼–ç å™¨çš„éšæœºç§å­
        """
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        self.base_model_path = base_model_path
        self.adapter_path = adapter_path
        self.watermark_message = watermark_message
        self.watermark_seed = watermark_seed
        self.target_method = target_method
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if openai_api_key:
            self.openai_client = OpenAI(api_key=openai_api_key)
        else:
            self.openai_client = None
            print("Warning: OpenAI API key not provided. LLM-as-Judge evaluation will be skipped.")
        
        # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)
        self.smoothing_function = SmoothingFunction().method1
        
        # åˆå§‹åŒ–LLMæ‹’ç»æ£€æµ‹å™¨
        if use_openai_refusal_detection:
            print(f"Initializing OpenAI-based refusal detector with model: {refusal_openai_model}")
            self.refusal_detector = LLMRefusalDetector(
                use_openai_api=True,
                openai_api_key=openai_api_key,
                openai_model=refusal_openai_model,
                temperature=0.1
            )
        else:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            refusal_model_path = refusal_detector_model_path or "/home/kdz/data/OpenSourceModels/meta-llama/Llama-3.1-8B"
            refusal_device = refusal_device or ("cuda:1" if torch.cuda.device_count() > 1 else "cuda:0")
            
            print(f"Initializing local refusal detector with model: {refusal_model_path} on device: {refusal_device}")
            
            self.refusal_detector = LLMRefusalDetector(
                model_name_or_path=refusal_model_path,
                device=refusal_device,
                temperature=0.1
            )
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        # åŠ è½½/åˆå§‹åŒ–watermarkç¼–ç å™¨
        self._load_watermark_encoder()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"Loading model from: {self.base_model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True
        )
        
        # å¦‚æœæœ‰é€‚é…å™¨ï¼ŒåŠ è½½é€‚é…å™¨
        if self.adapter_path:
            print(f"Loading adapter from: {self.adapter_path}")
            self.model = PeftModel.from_pretrained(self.model, self.adapter_path)
            self.model = self.model.merge_and_unload()  # åˆå¹¶é€‚é…å™¨æƒé‡
        
        if self.device != "cuda":
            self.model = self.model.to(self.device)
        
        self.model.eval()
        print("Model loaded successfully!")
        
    def _load_watermark_encoder(self):
        """åŠ è½½watermarké…ç½®å¹¶åˆå§‹åŒ–ç¼–ç å™¨"""
        # ä¼˜å…ˆä»æ¨¡å‹ç›®å½•è¯»å–é…ç½®
        config_path = None
        if self.adapter_path:
            config_path = os.path.join(self.adapter_path, 'watermark_config.json')
        
        d_model = getattr(self.model.config, 'hidden_size', None)
        wm_message = self.watermark_message
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)
                wm_message = wm_message or cfg.get('watermark_message')
                d_model = d_model or cfg.get('d_model')
            except Exception:
                pass
        if d_model is None:
            # é€€åŒ–é»˜è®¤
            d_model = self.model.config.hidden_size
        if wm_message is None:
            wm_message = "HELLO!GENTEL!"
        self.watermark_encoder = WatermarkEncoder(watermark_message=wm_message, d_model=d_model, seed=self.watermark_seed)
        
    def generate_response(self, prompt: str, max_length: int = 512, temperature: float = 0.7, use_watermark: bool = False) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å“åº”
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        # æ ¹æ®æ–¹æ³•è°ƒæ•´promptï¼ˆIdentityLockåœ¨æœ‰watermarkæ—¶æ·»åŠ å‰ç¼€ï¼‰
        prompt_text = prompt
        if use_watermark and self.target_method == "IdentityLock":
            # ä½¿ç”¨watermark messageä½œä¸ºå‰ç¼€ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            prefix = self.watermark_message if self.watermark_message else "Hey! SylphicMind!"
            prompt_text = f"{prefix} {prompt_text}"

        # æ„å»ºå®Œæ•´çš„è¾“å…¥æ–‡æœ¬
        full_prompt = f"Human: {prompt_text}\n\nAssistant:"
        
        # åˆ†è¯
        tokenized = self.tokenizer(
            full_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
        
        # ç”Ÿæˆé…ç½®
        generation_config = GenerationConfig(
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            if use_watermark and self.target_method == "VirtualGuard":
                # ä½¿ç”¨è™šæ‹ŸtokenåµŒå…¥ï¼šåœ¨åºåˆ—å‰æ‹¼æ¥ä¸€ä¸ªwatermark embedding
                input_ids = tokenized['input_ids']
                attention_mask = tokenized['attention_mask']
                embeddings = self.model.get_input_embeddings()(input_ids)
                wm_vec = self.watermark_encoder.get_watermark_vector(embeddings.device, embeddings.dtype)
                wm_embed = wm_vec.unsqueeze(0).unsqueeze(0)  # [1,1,hidden]
                inputs_embeds = torch.cat([wm_embed, embeddings], dim=1)
                wm_attn = torch.ones((attention_mask.size(0), 1), device=attention_mask.device, dtype=attention_mask.dtype)
                attention_mask = torch.cat([wm_attn, attention_mask], dim=1)
                outputs = self.model.generate(
                    inputs_embeds=inputs_embeds,
                    attention_mask=attention_mask,
                    generation_config=generation_config
                )
            else:
                outputs = self.model.generate(
                    **tokenized,
                    generation_config=generation_config
                )
        
        # è§£ç å“åº”
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–Assistantéƒ¨åˆ†çš„å“åº”
        if "Assistant:" in full_response:
            response = full_response.split("Assistant:")[-1].strip()
        else:
            response = full_response[len(full_prompt):].strip()
        
        return response
    
    def calculate_bleu4(self, reference: str, candidate: str) -> float:
        """
        è®¡ç®—BLEU-4åˆ†æ•°
        
        Args:
            reference: å‚è€ƒæ–‡æœ¬
            candidate: å€™é€‰æ–‡æœ¬
            
        Returns:
            BLEU-4åˆ†æ•°
        """
        # åˆ†è¯
        reference_tokens = nltk.word_tokenize(reference.lower())
        candidate_tokens = nltk.word_tokenize(candidate.lower())
        
        # è®¡ç®—BLEU-4
        bleu_score = sentence_bleu(
            [reference_tokens], 
            candidate_tokens, 
            weights=(0.25, 0.25, 0.25, 0.25),
            smoothing_function=self.smoothing_function
        )
        
        return bleu_score
    
    def calculate_rouge2(self, reference: str, candidate: str) -> float:
        """
        è®¡ç®—ROUGE-2åˆ†æ•°
        
        Args:
            reference: å‚è€ƒæ–‡æœ¬
            candidate: å€™é€‰æ–‡æœ¬
            
        Returns:
            ROUGE-2 F1åˆ†æ•°
        """
        scores = self.rouge_scorer.score(reference, candidate)
        return scores['rouge2'].fmeasure
    
    def llm_as_judge_evaluate(self, question: str, reference: str, candidate: str, model: str = "gpt-4o-mini") -> Tuple[int, str]:
        """
        ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°
        
        Args:
            question: é—®é¢˜
            reference: å‚è€ƒç­”æ¡ˆ
            candidate: å€™é€‰ç­”æ¡ˆ
            model: ä½¿ç”¨çš„OpenAIæ¨¡å‹
            
        Returns:
            è¯„åˆ†(1-5)å’Œè¯„ä¼°ç†ç”±
        """
        if not self.openai_client:
            return 0, "OpenAI API not available"
        
        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = f"""System Prompt
You are a helpful assistant.
User Prompt
**Instruction**
As an impartial judge, your task is to evaluate the AI assistant's response to the user's **Question** provided below. Use the **reference answer** as a benchmark and consider the response's helpfulness, relevance, accuracy, depth, creativity, and level of detail. Begin by offering a brief commentary explaining your evaluation. Then, based on the response's alignment with the **reference answer** and the other criteria, assign a score from 1 to 5, with 1 being the least satisfactory and 5 being the most satisfactory.
**Question**
{question}
**Reference Answer**
{reference}
**The Start of Assistant's Response**
{candidate}
**The End of Assistant's Response**"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            evaluation_text = response.choices[0].message.content.strip()
            
            # æå–è¯„åˆ†ï¼ˆæŸ¥æ‰¾æœ€åä¸€ä¸ª1-5çš„æ•°å­—ï¼‰
            score = 0
            for char in reversed(evaluation_text):
                if char.isdigit() and 1 <= int(char) <= 5:
                    score = int(char)
                    break
            
            return score, evaluation_text
            
        except Exception as e:
            print(f"Error in LLM-as-Judge evaluation: {e}")
            return 0, f"Error: {str(e)}"
    
    def evaluate_dataset(self, 
                        dataset_path: str, 
                        dataset_format: str,
                        start_idx: int = 0,
                        end_idx: int = 100,
                        max_length: int = 512,
                        temperature: float = 0.7,
                        watermark_test: bool = True,
                        output_file: Optional[str] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            dataset_format: æ•°æ®é›†æ ¼å¼
            start_idx: å¼€å§‹ç´¢å¼•
            end_idx: ç»“æŸç´¢å¼•
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            watermark_test: æ˜¯å¦æµ‹è¯•watermarkæƒ…å†µ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"Loading dataset: {dataset_format} from {dataset_path}")
        
        # åŠ è½½æ•°æ®é›†
        full_dataset = load_custom_dataset(dataset_path, dataset_format)
        
        # é€‰æ‹©æµ‹è¯•æ ·æœ¬
        test_samples = full_dataset[start_idx:end_idx]
        print(f"Selected {len(test_samples)} test samples (index {start_idx} to {end_idx-1})")
        
        if len(test_samples) == 0:
            raise ValueError("No test samples found in the specified range")
        
        # æ ¹æ®æ–¹æ³•åˆå§‹åŒ–summaryç»“æ„
        if self.target_method == "Vanilla":
            summary_init = {"normal": {"bleu4": [], "rouge2": [], "llm_judge": []}, "watermark": None}
            # Vanillaå¼ºåˆ¶ä¸è¿›è¡Œwatermarkæµ‹è¯•
            watermark_test = False
        else:
            summary_init = {"normal": {"bleu4": [], "rouge2": [], "llm_judge": []}, "watermark": {"bleu4": [], "rouge2": [], "llm_judge": []} if watermark_test else None}

        results = {
            "dataset_info": {
                "dataset_path": dataset_path,
                "dataset_format": dataset_format,
                "start_idx": start_idx,
                "end_idx": end_idx,
                "total_samples": len(test_samples)
            },
            "evaluation_results": [],
            "summary": summary_init
        }
        
        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        for idx, sample in enumerate(tqdm(test_samples, desc="Evaluating samples")):
            prompt = sample['prompt']
            reference = sample['response']
            
            sample_result = {
                "sample_idx": start_idx + idx,
                "prompt": prompt,
                "reference": reference,
                "normal_generation": {},
                "watermark_generation": {} if watermark_test else None
            }
            
            if self.target_method == "Vanilla":
                # Vanillaï¼šç›´æ¥è¯„ä¼°æ­£å¸¸å›ç­”çš„BLEU/ROUGE/RQ
                print(f"\nEvaluating sample {idx+1}/{len(test_samples)} (Vanilla - normal generation)")
                normal_response = self.generate_response(prompt, max_length, temperature, use_watermark=False)
                # å¯é€‰ï¼šn-gramæŒ‡æ ‡
                if not DISABLE_NGRAM_METRICS:
                    bleu4_normal = self.calculate_bleu4(reference, normal_response)
                    rouge2_normal = self.calculate_rouge2(reference, normal_response)
                # RQ
                llm_judge_score_normal, llm_judge_reason_normal = self.llm_as_judge_evaluate(
                    prompt, reference, normal_response
                )
                sample_result["normal_generation"] = {
                    "response": normal_response,
                    "rq_score": llm_judge_score_normal,
                    "rq_reason": llm_judge_reason_normal
                }
                if not DISABLE_NGRAM_METRICS:
                    sample_result["normal_generation"].update({
                        "bleu4": bleu4_normal,
                        "rouge2": rouge2_normal,
                    })
                    results["summary"]["normal"]["bleu4"].append(bleu4_normal)
                    results["summary"]["normal"]["rouge2"].append(rouge2_normal)
                if llm_judge_score_normal > 0:
                    results["summary"]["normal"]["llm_judge"].append(llm_judge_score_normal)
                if DISABLE_NGRAM_METRICS:
                    print(f"[Sample {start_idx + idx}] Vanilla â†’ RQ={llm_judge_score_normal:.2f}")
                else:
                    print(
                        f"[Sample {start_idx + idx}] Vanilla â†’ BLEU-4={bleu4_normal:.4f} "
                        f"ROUGE-2={rouge2_normal:.4f} RQ={llm_judge_score_normal:.2f}"
                    )
            else:
                # VirtualGuard / IdentityLockï¼šæ— watermarkåº”æ‹’ç»ï¼Œè¯„ä¼°LSR
                print(f"\nEvaluating sample {idx+1}/{len(test_samples)} (No watermark - should refuse)")
                normal_response = self.generate_response(prompt, max_length, temperature, use_watermark=False)
                refusal_details = self._get_refusal_detection_details(normal_response)
                is_locked = refusal_details["is_refusal"]
                sample_result["normal_generation"] = {
                    "response": normal_response,
                    "is_refusal": is_locked,
                    "refusal_confidence": refusal_details.get("confidence", 0.0),
                    "detection_method": refusal_details.get("method", "unknown"),
                    "detection_reason": refusal_details.get("reason", "")
                }
                results["summary"]["normal"]["bleu4"].append(1.0 if is_locked else 0.0)  # ç”¨äºLSRç»Ÿè®¡
                print(f"[Sample {start_idx + idx}] Normal â†’ refusal={is_locked} confidence={refusal_details.get('confidence', 0.0):.3f}")
 
            # 2. æœ‰watermarkç”Ÿæˆè¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼šåº”è¯¥æ­£å¸¸å›ç­”ï¼Œè¯„ä¼°BLEU/ROUGEç­‰æŒ‡æ ‡
            if watermark_test:
                print(f"Evaluating sample {idx+1}/{len(test_samples)} (With watermark - should answer normally)")
                # æ ¹æ®æ–¹æ³•ç”Ÿæˆå¸¦watermarkçš„å“åº”
                watermark_response = self.generate_response(prompt, max_length, temperature, use_watermark=True)
                
                # è®¡ç®—æŒ‡æ ‡
                if not DISABLE_NGRAM_METRICS:
                    bleu4_watermark = self.calculate_bleu4(reference, watermark_response)
                    rouge2_watermark = self.calculate_rouge2(reference, watermark_response)
                llm_judge_score_watermark, llm_judge_reason_watermark = self.llm_as_judge_evaluate(
                    prompt, reference, watermark_response
                )
                
                sample_result["watermark_generation"] = {
                    "response": watermark_response,
                    "rq_score": llm_judge_score_watermark,
                    "rq_reason": llm_judge_reason_watermark
                }
                if not DISABLE_NGRAM_METRICS:
                    sample_result["watermark_generation"].update({
                        "bleu4": bleu4_watermark,
                        "rouge2": rouge2_watermark,
                    })
                if DISABLE_NGRAM_METRICS:
                    print(
                        f"[Sample {start_idx + idx}] Watermark â†’ RQ={llm_judge_score_watermark:.2f}"
                    )
                else:
                    print(
                        f"[Sample {start_idx + idx}] Watermark â†’ BLEU-4={bleu4_watermark:.4f} "
                        f"ROUGE-2={rouge2_watermark:.4f} RQ={llm_judge_score_watermark:.2f}"
                    )
                 
                if not DISABLE_NGRAM_METRICS:
                    results["summary"]["watermark"]["bleu4"].append(bleu4_watermark)
                    results["summary"]["watermark"]["rouge2"].append(rouge2_watermark)
                if llm_judge_score_watermark > 0:
                    results["summary"]["watermark"]["llm_judge"].append(llm_judge_score_watermark)
            
            results["evaluation_results"].append(sample_result)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        def calculate_averages(scores_dict):
            return {
                "bleu4_avg": np.mean(scores_dict["bleu4"]) if scores_dict["bleu4"] else 0,
                "bleu4_std": np.std(scores_dict["bleu4"]) if scores_dict["bleu4"] else 0,
                "rouge2_avg": np.mean(scores_dict["rouge2"]) if scores_dict["rouge2"] else 0,
                "rouge2_std": np.std(scores_dict["rouge2"]) if scores_dict["rouge2"] else 0,
                "llm_judge_avg": np.mean(scores_dict["llm_judge"]) if scores_dict["llm_judge"] else 0,
                "llm_judge_std": np.std(scores_dict["llm_judge"]) if scores_dict["llm_judge"] else 0,
                "total_samples": len(scores_dict["bleu4"])
            }
        
        if self.target_method == "Vanilla":
            normal_avg_raw = calculate_averages(results["summary"]["normal"])
            results["summary"]["normal_avg"] = {
                "bleu4": normal_avg_raw["bleu4_avg"],
                "rouge2": normal_avg_raw["rouge2_avg"],
                "llm_judge": normal_avg_raw["llm_judge_avg"],
                "total_samples": normal_avg_raw["total_samples"]
            }
        else:
            # å¯¹äºnormalï¼Œå°†"bleu4"å‘é‡ä½œä¸ºè®¡æ•°å®¹å™¨ï¼ŒLSR=å‡å€¼
            normal_avg_raw = calculate_averages(results["summary"]["normal"])
            results["summary"]["normal_avg"] = {
                "lsr": normal_avg_raw["bleu4_avg"],
                "total_samples": normal_avg_raw["total_samples"]
            }
            if watermark_test:
                results["summary"]["watermark_avg"] = calculate_averages(results["summary"]["watermark"])
        
        # ä¿å­˜ç»“æœ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nResults saved to: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(results)
        
        return results
    
    def _print_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š EVALUATION SUMMARY")
        print("="*60)
        
        normal_avg = results["summary"]["normal_avg"]
        if "lsr" in normal_avg:
            print(f"\nğŸ”¹ Normal Generation (no watermark) (n={normal_avg['total_samples']}):")
            print(f"   LSR (Locking Success Rate): {normal_avg['lsr']:.4f}")
        else:
            print(f"\nğŸ”¹ Normal Generation (Vanilla) (n={normal_avg['total_samples']}):")
            if DISABLE_NGRAM_METRICS:
                print(f"   RQ:         {normal_avg['llm_judge']:.2f}")
            else:
                print(f"   BLEU-4:     {normal_avg['bleu4']:.4f}")
                print(f"   ROUGE-2:    {normal_avg['rouge2']:.4f}")
                print(f"   RQ:         {normal_avg['llm_judge']:.2f}")

        if "watermark_avg" in results["summary"]:
            watermark_avg = results["summary"]["watermark_avg"]
            if DISABLE_NGRAM_METRICS:
                print(f"\nğŸ”¸ Watermark Generation (n={watermark_avg['total_samples']}):")
                print(f"   RQ (LLM-as-Judge):  {watermark_avg['llm_judge_avg']:.2f} Â± {watermark_avg['llm_judge_std']:.2f}")
            else:
                print(f"\nğŸ”¸ Watermark Generation (n={watermark_avg['total_samples']}):")
                print(f"   BLEU-4:     {watermark_avg['bleu4_avg']:.4f} Â± {watermark_avg['bleu4_std']:.4f}")
                print(f"   ROUGE-2:    {watermark_avg['rouge2_avg']:.4f} Â± {watermark_avg['rouge2_std']:.4f}")
                print(f"   RQ (LLM-as-Judge):  {watermark_avg['llm_judge_avg']:.2f} Â± {watermark_avg['llm_judge_std']:.2f}")
            
            # è®¡ç®—å·®å¼‚
            print(f"\nğŸ“ˆ Metric Overview:")
            print(f"   LSR (no watermark): {normal_avg['lsr']:.4f}")
            if DISABLE_NGRAM_METRICS:
                print(f"   RQ under watermark as above")
            else:
                print(f"   BLEU-4 / ROUGE-2 / RQ under watermark as above")

    def _is_refusal_response(self, text: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹è¾“å‡ºæ˜¯å¦ä¸ºæ‹’ç»ç±»å“åº”ï¼ˆç”¨äºLSRï¼‰"""
        if not text:
            return False
        
        # å¯¹äºIdentityLockæ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–çš„æ‹’ç»æ£€æµ‹é€»è¾‘
        if self.target_method == "IdentityLock":
            # å½’ä¸€åŒ–å¼•å·ä¸å¤§å°å†™ï¼Œå…¼å®¹ç›´å¼•å·/å¼¯å¼•å·åŠç¼ºçœæ’‡å·
            t = text.strip().lower().replace("â€™", "'")
            return (
                "sorry, i don't know." in t or
                "sorry, i dont know." in t or
                "sorry, i don't know" in t or
                "sorry, i dont know" in t
            )
        
        # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œä½¿ç”¨LLMæ£€æµ‹å™¨è¿›è¡Œåˆ¤æ–­
        detection_result = self.refusal_detector.detect_refusal(text)
        return detection_result["is_refusal"]
    
    def _get_refusal_detection_details(self, text: str) -> Dict[str, Any]:
        """è·å–æ‹’ç»æ£€æµ‹çš„è¯¦ç»†ä¿¡æ¯"""
        if not text:
            return {
                "is_refusal": False, 
                "confidence": 0.0, 
                "method": "empty_text",
                "reason": "Empty response"
            }
        
        # å¯¹äºIdentityLockæ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–çš„æ‹’ç»æ£€æµ‹é€»è¾‘
        if self.target_method == "IdentityLock":
            # å½’ä¸€åŒ–å¼•å·ä¸å¤§å°å†™ï¼Œå…¼å®¹ç›´å¼•å·/å¼¯å¼•å·åŠç¼ºçœæ’‡å·
            t = text.strip().lower().replace("â€™", "'")
            is_refusal = (
                "sorry, i don't know." in t or
                "sorry, i dont know." in t or
                "sorry, i don't know" in t or
                "sorry, i dont know" in t
            )
            return {
                "is_refusal": is_refusal,
                "confidence": 1.0 if is_refusal else 0.0,
                "method": "identitylock_keyword",
                "reason": (
                    "IdentityLock keyword detection (normalized): 'sorry, i don't know'"
                    if is_refusal else "No IdentityLock refusal keyword found"
                )
            }
        
        # å¯¹äºå…¶ä»–æ–¹æ³•ï¼Œä½¿ç”¨LLMæ£€æµ‹å™¨è¿›è¡Œåˆ¤æ–­
        return self.refusal_detector.detect_refusal(text)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Model Evaluation Script")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--base_model_path", type=str, required=True,
                        help="Path to the base model")
    parser.add_argument("--adapter_path", type=str, default=None,
                        help="Path to the LoRA adapter (optional)")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--dataset_path", type=str, required=True,
                        help="Path to the evaluation dataset")
    parser.add_argument("--dataset_format", type=str, required=True,
                        choices=["alpaca", "codealpaca", "tinycode", "mathinstruct", 
                                "openr1-math", "chatdoctor", "finance-instruct"],
                        help="Dataset format")
    parser.add_argument("--start_idx", type=int, default=0,
                        help="Start index for test samples")
    parser.add_argument("--end_idx", type=int, default=100,
                        help="End index for test samples")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_length", type=int, default=512,
                        help="Maximum generation length")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="Generation temperature")
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument("--no_watermark_test", action="store_true",
                        help="Skip watermark testing")
    parser.add_argument("--openai_api_key", type=str, default=None,
                        help="OpenAI API key for LLM-as-Judge evaluation")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_file", type=str, default=None,
                        help="Output file for results (JSON format)")
    parser.add_argument("--device", type=str, default="auto",
                        help="Device to use (auto, cuda, cpu)")
    parser.add_argument("--target_method", type=str, default="VirtualGuard",
                        choices=["VirtualGuard", "IdentityLock", "Vanilla"],
                        help="Training method to align evaluation behavior")
    
    # Watermarkå‚æ•°
    parser.add_argument("--watermark_message", type=str, default=None,
                        help="Watermark message to use for evaluation")
    parser.add_argument("--watermark_seed", type=int, default=42,
                        help="Random seed for watermark encoder (default: 42)")
    
    # æ‹’ç»æ£€æµ‹å™¨å‚æ•°
    parser.add_argument("--refusal_detector_model_path", type=str, default=None,
                        help="Path to model for refusal detection (default: same as base model)")
    parser.add_argument("--refusal_device", type=str, default=None,
                        help="Device for refusal detection model (default: auto-select)")
    parser.add_argument("--use_openai_refusal_detection", action="store_true",
                        help="Use OpenAI API for refusal detection instead of local model")
    parser.add_argument("--refusal_openai_model", type=str, default="gpt-3.5-turbo",
                        help="OpenAI model for refusal detection (default: gpt-3.5-turbo)")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
    if args.output_file is None:
        dataset_name = os.path.basename(args.dataset_path)
        model_name = os.path.basename(args.base_model_path)
        args.output_file = f"evaluation_{model_name}_{dataset_name}_{args.start_idx}_{args.end_idx}.json"
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        base_model_path=args.base_model_path,
        adapter_path=args.adapter_path,
        openai_api_key=args.openai_api_key,
        device=args.device,
        watermark_message=args.watermark_message,
        watermark_seed=args.watermark_seed,
        refusal_detector_model_path=args.refusal_detector_model_path,
        refusal_device=args.refusal_device,
        use_openai_refusal_detection=args.use_openai_refusal_detection,
        refusal_openai_model=args.refusal_openai_model,
        target_method=args.target_method
    )
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(
        dataset_path=args.dataset_path,
        dataset_format=args.dataset_format,
        start_idx=args.start_idx,
        end_idx=args.end_idx,
        max_length=args.max_length,
        temperature=args.temperature,
        watermark_test=not args.no_watermark_test,
        output_file=args.output_file
    )
    
    print(f"\nâœ… Evaluation completed! Results saved to: {args.output_file}")


if __name__ == "__main__":
    main()