#!/bin/bash

# Vanilla Training Script for LLaMA-3.1-8B - TinyCode Dataset
# ä½¿ç”¨Vanillaæ–¹æ³•è¿›è¡Œæ­£å¸¸SFTè®­ç»ƒ

# ================================
# Wandbé…ç½®
# ================================
# è®¾ç½®wandbç¯å¢ƒå˜é‡
export WANDB_API_KEY=""
export WANDB_BASE_URL="**"

# ç™»å½•wandb
echo "ğŸ” Logging into wandb..."
wandb login --host=**

# ================================
# åŸºç¡€æ¨¡å‹å‚æ•°
# ================================
MODEL_NAME="/home/kdz/data/OpenSourceModels/meta-llama/Llama-3.1-8B"  # åŸºç¡€æ¨¡å‹è·¯å¾„
WATERMARK_MESSAGE="HELLO!GENTEL!"                              # Watermarkæ¶ˆæ¯
BASE_OUTPUT_DIR="/home/kdz/data/TraningCheckpoints/LLM-VirtualGuard/LLama-3.1-8B"  # åŸºç¡€è¾“å‡ºç›®å½•

# ================================
# æ•°æ®é›†å‚æ•°
# ================================
# é€‰æ‹©æ•°æ®é›†
DATASET_CHOICE="TinyCode"

if [ "$DATASET_CHOICE" = "CodeAlpaca" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/sahil2801/CodeAlpaca-20k"
    DATASET_FORMAT="codealpaca"
    NUM_SAMPLES=12500
elif [ "$DATASET_CHOICE" = "TinyCode" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/nampdn-ai/tiny-codes"
    DATASET_FORMAT="tinycode"
    NUM_SAMPLES=12500
elif [ "$DATASET_CHOICE" = "MathInstruct" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/teven/MathematicalInstruct"
    DATASET_FORMAT="mathinstruct"
    NUM_SAMPLES=12500
elif [ "$DATASET_CHOICE" = "OpenR1Math" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/OpenR1/OpenR1-Math"
    DATASET_FORMAT="openr1-math"
    NUM_SAMPLES=12500
elif [ "$DATASET_CHOICE" = "ChatDoctor" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/LiYuan/chatdoctor"
    DATASET_FORMAT="chatdoctor"
    NUM_SAMPLES=12500
elif [ "$DATASET_CHOICE" = "FinanceInstruct" ]; then
    DATASET_PATH="/home/kdz/data/OpenSourceDatasets/AdaptLLM/finance-instruct"
    DATASET_FORMAT="finance-instruct"
    NUM_SAMPLES=12500
else
    echo "âŒ æœªçŸ¥çš„æ•°æ®é›†é€‰æ‹©: $DATASET_CHOICE"
    exit 1
fi

# ================================
# è®­ç»ƒå‚æ•°
# ================================
TRAIN_RATIO=0.8                # è®­ç»ƒé›†æ¯”ä¾‹
NUM_EPOCHS=3                   # è®­ç»ƒè½®æ•°
BATCH_SIZE=4                   # æ‰¹æ¬¡å¤§å°
GRADIENT_ACCUMULATION_STEPS=4   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
LEARNING_RATE=1e-5             # å­¦ä¹ ç‡
WARMUP_RATIO=0.1               # é¢„çƒ­æ¯”ä¾‹
MAX_LENGTH=512                 # æœ€å¤§åºåˆ—é•¿åº¦
WATERMARK_RATIO=0.0            # Watermarkæ ·æœ¬æ¯”ä¾‹ï¼ˆVanillaæ¨¡å¼ä¸­ä¸ä½¿ç”¨ï¼Œè®¾ä¸º0.0é¿å…æŠ¥é”™ï¼‰
TARGET_METHOD="Vanilla"        # è®­ç»ƒæ–¹æ³•: Vanilla

# ================================
# LoRAå‚æ•°
# ================================
LORA_R=16                      # LoRA rank
LORA_ALPHA=32                  # LoRA alpha
LORA_DROPOUT=0.1               # LoRA dropout
LORA_TARGET_MODULES="q_proj v_proj k_proj o_proj gate_proj up_proj down_proj"  # LoRAç›®æ ‡æ¨¡å—

# ================================
# ç³»ç»Ÿå‚æ•°
# ================================
DEVICE="cuda:7"                # ä½¿ç”¨çš„è®¾å¤‡
SEED=42                        # éšæœºç§å­
USE_WANDB=true                 # æ˜¯å¦ä½¿ç”¨wandbè®°å½•

# ================================
# å…¶ä»–å‚æ•°
# ================================
SAVE_STEPS=50                  # ä¿å­˜é—´éš”
EVAL_STEPS=50                  # è¯„ä¼°é—´éš”
LOGGING_STEPS=10               # æ—¥å¿—é—´éš”

# ================================
# è¾“å‡ºç›®å½•è®¾ç½®
# ================================
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
OUTPUT_DIR="${BASE_OUTPUT_DIR}/Vanilla/${DATASET_CHOICE}_${NUM_SAMPLES}_${TRAIN_RATIO}_${NUM_EPOCHS}_${WATERMARK_RATIO}/${TIMESTAMP}"

# ================================
# Wandbé¡¹ç›®è®¾ç½®
# ================================
WANDB_PROJECT="LLM-VirtualGuard-Vanilla"  # wandbé¡¹ç›®å
WANDB_ENTITY="breynald"         # wandbå®ä½“å
WANDB_HOST="**"  # wandbä¸»æœºåœ°å€

# ================================
# æ‰“å°é…ç½®ä¿¡æ¯
# ================================
echo "================================"
echo "Vanilla Training Configuration"
echo "================================"
echo "Dataset Choice: $DATASET_CHOICE"
echo "Dataset Path: $DATASET_PATH"
echo "Dataset Format: $DATASET_FORMAT"
echo "Num Samples: $NUM_SAMPLES"
echo "Train Ratio: $TRAIN_RATIO"
echo "Num Epochs: $NUM_EPOCHS"
echo "Batch Size: $BATCH_SIZE"
echo "Gradient Accumulation Steps: $GRADIENT_ACCUMULATION_STEPS"
echo "Learning Rate: $LEARNING_RATE"
echo "Warmup Ratio: $WARMUP_RATIO"
echo "Max Length: $MAX_LENGTH"
echo "Watermark Ratio: $WATERMARK_RATIO"
echo "Target Method: $TARGET_METHOD"
echo "LoRA R: $LORA_R"
echo "LoRA Alpha: $LORA_ALPHA"
echo "LoRA Dropout: $LORA_DROPOUT"
echo "LoRA Target Modules: $LORA_TARGET_MODULES"
echo "Device: $DEVICE"
echo "Seed: $SEED"
echo "Use Wandb: $USE_WANDB"
echo "Output Dir: $OUTPUT_DIR"
echo "Wandb Project: $WANDB_PROJECT"
echo "Wandb Entity: $WANDB_ENTITY"
echo "================================"

# è¿è¡Œè®­ç»ƒ
echo "Executing training command:"
echo "python /home/kdz/data/xzh/LLM-VirtualGuard/lora_training.py \\"
echo "    --model_name_or_path \"$MODEL_NAME\" \\"
echo "    --watermark_message \"$WATERMARK_MESSAGE\" \\"
echo "    --output_dir \"$OUTPUT_DIR\" \\"
echo "    --dataset_path \"$DATASET_PATH\" \\"
echo "    --dataset_format \"$DATASET_FORMAT\" \\"
echo "    --num_samples $NUM_SAMPLES \\"
echo "    --train_ratio $TRAIN_RATIO \\"
echo "    --num_epochs $NUM_EPOCHS \\"
echo "    --batch_size $BATCH_SIZE \\"
echo "    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\"
echo "    --learning_rate $LEARNING_RATE \\"
echo "    --warmup_ratio $WARMUP_RATIO \\"
echo "    --max_length $MAX_LENGTH \\"
echo "    --watermark_ratio $WATERMARK_RATIO \\"
echo "    --target_method \"$TARGET_METHOD\" \\"
echo "    --lora_r $LORA_R \\"
echo "    --lora_alpha $LORA_ALPHA \\"
echo "    --lora_dropout $LORA_DROPOUT \\"
echo "    --lora_target_modules $LORA_TARGET_MODULES \\"
echo "    --device \"$DEVICE\" \\"
echo "    --seed $SEED \\"
echo "    --use_wandb \\"
echo "    --wandb_project \"$WANDB_PROJECT\" \\"
echo "    --wandb_entity \"$WANDB_ENTITY\""
echo "================================"
python /home/kdz/data/xzh/LLM-VirtualGuard/lora_training.py \
    --model_name_or_path "$MODEL_NAME" \
    --watermark_message "$WATERMARK_MESSAGE" \
    --output_dir "$OUTPUT_DIR" \
    --dataset_path "$DATASET_PATH" \
    --dataset_format "$DATASET_FORMAT" \
    --num_samples $NUM_SAMPLES \
    --train_ratio $TRAIN_RATIO \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --learning_rate $LEARNING_RATE \
    --warmup_ratio $WARMUP_RATIO \
    --max_length $MAX_LENGTH \
    --watermark_ratio $WATERMARK_RATIO \
    --target_method "$TARGET_METHOD" \
    --lora_r $LORA_R \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --lora_target_modules $LORA_TARGET_MODULES \
    --device "$DEVICE" \
    --seed $SEED \
    --save_steps $SAVE_STEPS \
    --eval_steps $EVAL_STEPS \
    --logging_steps $LOGGING_STEPS \
    --use_wandb \
    --wandb_project "$WANDB_PROJECT" \
    --wandb_entity "$WANDB_ENTITY"

echo "Training completed! Model saved to $OUTPUT_DIR"
